{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Affiche le répertoire courant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Changer de répertoire\n",
    "os.chdir(\"C:/Users/jcluy/Python/detection-anomalies-aws-mlflow\")\n",
    "\n",
    "# Vérifier qu'on est bien dans le bon dossier\n",
    "print(\"Répertoire actuel :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.FonctionsPerso import *\n",
    "import pandas as pd\n",
    "\n",
    "# Définir le chemin proprement\n",
    "base_dir = \"C:/Users/jcluy/Python/detection-anomalies-aws-mlflow\"\n",
    "file_path = os.path.join(base_dir, \"data\", \"raw\", \"creditcard.csv\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "# Vérifier le chargement\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afficher_informations_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les valeurs manquantes\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]  # Affiche uniquement les colonnes avec des NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les doublons\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Nombre de doublons : {duplicates}\")\n",
    "\n",
    "# Supprimer les doublons si nécessaire\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/processed/creditcard_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sélectionner les colonnes numériques (hors labels comme 'Class' si présent)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Déterminer le nombre de lignes et colonnes pour les subplots\n",
    "n_cols = 3  # Fixe le nombre de colonnes à 3\n",
    "n_rows = int(np.ceil(len(num_cols) / n_cols))  # Calcule dynamiquement le nombre de lignes\n",
    "\n",
    "# Créer les subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 3)) \n",
    "axes = axes.flatten()  # Aplatir la grille en 1D pour éviter les erreurs d'index\n",
    "\n",
    "# Boucle pour tracer les histogrammes\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(df[col], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution de {col}\")\n",
    "\n",
    "# Supprimer les subplots inutilisés s'il y en a\n",
    "for j in range(i + 1, len(axes)):  \n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_dfs = {}\n",
    "\n",
    "# Déterminer le nombre de lignes et colonnes pour les subplots\n",
    "n_cols = 3  # Fixe le nombre de colonnes à 3\n",
    "n_rows = int(np.ceil(len(num_cols) / n_cols))  # Calcule dynamiquement le nombre de lignes\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df[num_cols]), columns=num_cols)\n",
    "    scaled_dfs[name] = scaled_df\n",
    "\n",
    "# Créer les subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 3)) \n",
    "axes = axes.flatten()  # Aplatir la grille en 1D pour éviter les erreurs d'index\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    for name, scaled_df in scaled_dfs.items():\n",
    "        sns.kdeplot(scaled_df[col], ax=axes[i], label=name, fill=True)\n",
    "    axes[i].set_title(f\"Normalisation de {col}\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Appliquer PCA sur les données normalisées\n",
    "pca = PCA(n_components=2)\n",
    "pca_transformed = pca.fit_transform(scaled_dfs[\"StandardScaler\"])  # Changer pour tester d'autres scalers\n",
    "\n",
    "# Visualiser la variance expliquée\n",
    "print(f\"Variance expliquée par les deux premières composantes : {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "# Visualisation du PCA\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(pca_transformed[:,0], pca_transformed[:,1], alpha=0.5)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Visualisation des données après PCA\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_full = PCA()\n",
    "pca_full.fit(scaled_dfs[\"StandardScaler\"])  # Changer pour tester un autre scaler\n",
    "\n",
    "# Calculer la variance expliquée cumulée\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Tracer le graphe\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.axhline(y=0.8, color='r', linestyle='--')  # Ligne rouge pour 80% de variance expliquée\n",
    "plt.xlabel(\"Nombre de composantes\")\n",
    "plt.ylabel(\"Variance expliquée cumulée\")\n",
    "plt.title(\"Choix du nombre de composantes pour la PCA\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = np.abs(pca_full.components_[0])\n",
    "feature_names = scaled_dfs[\"StandardScaler\"].columns\n",
    "\n",
    "# Afficher les features les plus importantes pour PC1\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "for i in range(10):\n",
    "    print(f\"{feature_names[sorted_idx[i]]}: {feature_importance[sorted_idx[i]]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les valeurs négatives par colonne\n",
    "neg_values = df[num_cols].lt(-1).sum()\n",
    "print(neg_values[neg_values > 0])  # Afficher uniquement les colonnes avec des valeurs < -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shifted = df.copy()\n",
    "\n",
    "for col in num_cols:\n",
    "    min_val = df_shifted[col].min()\n",
    "    if min_val < 0:  \n",
    "        df_shifted[col] += abs(min_val) + 1  # Décalage pour que la plus petite valeur soit 1\n",
    "\n",
    "# Appliquer la transformation logarithmique après le décalage\n",
    "df_log = np.log1p(df_shifted[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df.copy()\n",
    "for col in num_cols:\n",
    "    df_log[col] = np.log1p(df_log[col])  # log(x + 1) pour éviter log(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.describe()  # Vérifier les statistiques après la transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "np.warnings = warnings  # Correction de l'import\n",
    "\n",
    "# Séparer les features (X) et la target (y) avant la transformation\n",
    "X = df.drop(columns=[\"Class\"])  # Remplace \"Class\" par le nom exact de la cible\n",
    "y = df[\"Class\"]  # Garder y intact\n",
    "\n",
    "# Appliquer la transformation uniquement sur les features\n",
    "scaler = PowerTransformer()\n",
    "X_transformed = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.unique())  # Doit afficher uniquement [0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Sélectionner les features (X) et la target (y)\n",
    "# X = df_transformed.drop(columns=[\"Class\"])  # Remplace \"Class\" par la vraie colonne cible\n",
    "# y = df_transformed[\"Class\"]\n",
    "\n",
    "# Séparer en train (80%) et test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Vérification des tailles\n",
    "print(f\"Taille du jeu d'entraînement : {X_train.shape}\")\n",
    "print(f\"Taille du jeu de test : {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialiser et entraîner le modèle\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.unique())  # Vérifie les valeurs uniques\n",
    "print(y_train.value_counts())  # Affiche le nombre d’occurrences par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.set_experiment(\"Detection Anomalies\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Log des résultats\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    \n",
    "    # Sauvegarde du modèle\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "print(\"Expérience enregistrée dans MLflow ! 🎯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Définir l'expérience MLflow (elle sera visible dans MLflow UI)\n",
    "mlflow.set_experiment(\"Detection Anomalies\")\n",
    "\n",
    "print(\"MLflow est activé ! 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with mlflow.start_run():  # Démarrer un run MLflow\n",
    "    # Définir et entraîner le modèle\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Faire des prédictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 📌 Enregistrer les paramètres\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    \n",
    "    # 📌 Enregistrer la métrique d’accuracy\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    # 📌 Enregistrer le modèle\n",
    "    input_example = pd.DataFrame([X_train.iloc[0]])  # Exemple d'entrée\n",
    "    mlflow.sklearn.log_model(model, \"model\", input_example=input_example)\n",
    "\n",
    "print(\"Expérience et modèle enregistrés avec succès ! 🎯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlflow.get_tracking_uri())  # Voir où les logs sont enregistrés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = mlflow.search_experiments()\n",
    "for exp in experiments:\n",
    "    print(f\"Expérience : {exp.name} | ID : {exp.experiment_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog()  # Active l'auto-logging\n",
    "with mlflow.start_run():\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlflow.get_tracking_uri())  # Voir où les logs sont enregistrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mlflow_path = \"C:/Users/jcluy/mlruns/855462862201448649\"  # ID de ton expérience\n",
    "print(os.path.exists(mlflow_path))  # Vérifier si le dossier existe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans anaconda prompt\n",
    "# mlflow ui --backend-store-uri file:///C:/Users/jcluy/Python/detection-anomalies-aws-mlflow/mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 📌 Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 📌 Log des métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # 📌 Log du modèle\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "print(\"Métriques et modèle enregistrés avec succès ! 🎯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = mlflow.search_runs(order_by=[\"start_time desc\"])\n",
    "print(runs[[\"run_id\", \"metrics.accuracy\"]].head())  # Affiche les derniers runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow models serve -m \"mlruns/855462862201448649/44dc24312237424b9b341bed1ed4f903/artifacts/model\" -p 5001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# url = \"http://127.0.0.1:5001/invocations\"\n",
    "# data = {\"instances\": [[5.1, 3.5, 1.4, 0.2]]}  # Remplace par les vraies features\n",
    "# headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "# print(response.json())  # Prédiction du modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:5001/invocations\"\n",
    "\n",
    "# Générer un exemple avec 30 features (remplace par une vraie ligne du dataset)\n",
    "sample_input = np.random.rand(30).tolist()  # Exemple aléatoire\n",
    "\n",
    "data = {\"instances\": [sample_input]}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "print(response.json())  # Affiche la prédiction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
